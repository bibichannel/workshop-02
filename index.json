[{"uri":"https://bibichannel.github.io/workshop-02/","title":"Create a complete project with Terraform and AWS services","tags":[],"description":"","content":"Overview With Workshop 01, we guide you to develop and automatically deploy a static website on AWS cloud using services like Cloud9, S3, CodePipeline, CloudFront, Route53, etc.\nIn Workshop 02, we will explore how to provision infrastructure on AWS as code using an open-source tool, Terraform, developed by HashiCorp. Additionally, we will look at how to integrate AWS serverless services, CI/CD, monitoring, and management.\nLet\u0026rsquo;s take a look at the diagram below to get an overview of this workshop.\nContent 1. Introduction 2. Prerequisites 3. Setting up the repository on GitHub 4. Terraform and what you need to know 5. Provisioning infrastructure and testing 6. Cleaning up resources "},{"uri":"https://bibichannel.github.io/workshop-02/2-prerequisites/2.1-createiam/","title":"Create User Groups and IAM User","tags":[],"description":"","content":"Before creating an IAM User, we will create a User Group with the necessary policies. Creating user groups helps us easily control users centrally, classify users according to their roles in the organization, and flexibly grant and revoke permissions.\nNext, we assign the IAM User to the User Group we just created. This new user will inherit the permissions from the User Group.\nWe will use this user to authorize Terraform locally, allowing us to use Terraform to test configurations on AWS.\nAccess the IAM console:\nSelect User groups. Click on Create group. We will create a group named DevOpsGroup. In the Attach permissions policies section, select the AdministratorAccess policy. Click Create user group. Regarding selecting the AdministratorAccess policy. Since the goal of this workshop is not security, I will use this permission for convenience. I do not encourage you to use this wide-ranging privilege. Create a policy with the necessary limited permissions for better security.\nNext, we will create an IAM User. Select Users and click on Create user. Enter the user name as terraform. Select Next. In the Set permissions section, choose Add user to group. Select the DevOpsGroup we just created. Choose Next and then Create User. Interface after creating the user. Next, we will create an Access key for the user to use with AWS CLI. Click on the terraform user we just created, select the Security Credentials tab. Scroll down to the Access keys section and click Create access key. For Use case, select Command Line Interface (CLI). Check I understand the above recommendation and want to proceed to create an access key and select Next. In the Set description tag section, enter Check the terraform code call aws api. Click Create access key. Select Download access key .csv. You have now completed creating the User Group and IAM User. Let\u0026rsquo;s move on to the next step. "},{"uri":"https://bibichannel.github.io/workshop-02/1-introduce/","title":"Introduction","tags":[],"description":"","content":"\nLooking at the architecture, you can see that this is a fairly large project involving many AWS services:\nDeploy the application across 2 AZs to increase HA and load balance to applications via Application Load Balancer (ALB). Create a pipeline from CodePipeline for the streamlit app, deploying it to the serverless Elastic Container Service. Use CodeBuild to build images and store them in the Elastic Container Registry (ECR). Receive pipeline status notifications via email with EventBridge and Amazon SNS. Use Route53 for a custom domain. What if you had to manually set up these services? Or when you hand over this project to a team member, will they set it up correctly and precisely as you documented? Or when your boss sees that your project is very good and wants to apply this architecture to future projects, setting up such a large number of services makes it inevitable to make mistakes. That is why infrastructure as code was born. It helps us to quickly deploy and minimize human errors,\u0026hellip;\nThis is also my purpose in writing this article. Through this workshop, I hope to help you confidently set up a project with Terraform on the AWS Cloud platform.\nHere I will have 2 repositories on GitHub for the following purposes:\nThe first repository will store the Terraform code to provision to AWS via GitHub Actions. This will help me to downgrade versions whenever configuration changes cause errors through version management on GitHub. And it also allows us to work together as a team. The second repository will store the source code of the streamlit app. Whenever code is pushed to the repo, it will trigger a push event through a webhook for CodePipeline to be triggered, built, and deployed to ECS Fargate. "},{"uri":"https://bibichannel.github.io/workshop-02/5-provisioningtesting/5.1-local/","title":"Local","tags":[],"description":"","content":"Preparation When running locally, we need to have two additional files: terraform.tfvars and vars.tfbackend. These files will assign values to the environment variables defined in the variables.tf file in the root directory and configure the backend to store state in an S3 bucket.\nFor terraform.tfvars, you will have the following content: aws_region = \u0026#34;us-east-1\u0026#34; stage_name = \u0026#34;staging\u0026#34; project_name = \u0026#34;workshop-02\u0026#34; create_by = \u0026lt;Your name\u0026gt; github_repo = \u0026lt;GitHubOrg/GitHubRepo\u0026gt; github_branch = \u0026#34;main\u0026#34; codestar_connection_arn = \u0026lt;CodeStar Conneciton ARN\u0026gt; account_id = \u0026lt;Account ID AWS\u0026gt; The github_repo variable is the name of the GitHub repo containing the source code for the Streamlit app created in step 3.2. This variable is used to configure the stage source of Codepipeline services. The github_branch variable is the branch of the GitHub repo containing the source code of the Streamlit app you want to monitor. This variable is used to configure the stage source of Codepipeline services. The codestar_connection_arn variable contains the ARN of the Connection created in step 2.4. This variable is used to configure the stage source of Codepipeline services. The account_id variable contains your AWS account ID, which is used to create the role for the Amazon Simple Notification Service (SNS) service. In addition to the listed variables, I\u0026rsquo;ve provided placeholders for other common variables used across modules or for tagging resources. For vars.tfbackend, you will have the following content: bucket = \u0026lt;Bucket\u0026#39;s name\u0026gt; key = \u0026#34;terraform-staging/terraform.tfstate\u0026#34; region = \u0026#34;us-east-1\u0026#34; Configuring the backend for Terraform to store the state file in an S3 bucket requires declaring variables as above. Using a backend helps in collaborating on Terraform code, securing the state file, etc. The bucket variable contains the name of the bucket you use to store the backend as created in step 2.3. The key variable configures the prefix for the path to the Terraform file in the bucket. The region variable determines the region where your bucket is located. You can read more about setting up a backend for Terraform at backend s3. Open the main.tf file in the root directory of the source code and navigate to the locals block: locals { tags = { Project = var.project_name CreateBy = var.create_by Environment = var.stage_name } container_name = \u0026#34;streamlit-app\u0026#34; container_port = 8501 container_cpu = 512 container_memory = 1024 notification_email = \u0026#34;trung.lykhanh150901@gmail.com\u0026#34; } In addition to the tags list containing key:value pairs with values passed from the terraform.tfvars file, we are interested in the following variables: container_name is the variable used to name the container in the ECS services configuration. container_port is the port exposed by the container. container_cpu sets the CPU limit for Fargate task definitions. notification_email is the email registered with the SNS service to receive notifications of state changes from CodePipeline. Adjust the values of these variables to suit your needs.\nExecution After you have these two files, run the following commands one by one:\nterraform init -backend-config=\u0026#39;vars.tfbackend\u0026#39; This command will install plugins for the providers you have declared in the providers.tf file, initialize a Terraform environment in the current directory, and configure the backend using the parameters from the vars.tfbackend file through the -backend-config flag. It also loads the modules we\u0026rsquo;ve written.\nThe following image shows the output after running this command:\nterraform validate This command is used to check if all your Terraform configuration files have correct syntax and are valid without making any actual changes to the infrastructure. It helps you catch syntax errors or logic errors before applying any changes.\nThe following image shows the output after running this command:\nterraform plan This command generates a plan to perform changes on the infrastructure based on the Terraform configuration and the current state without actually making any changes. It allows you to preview what will happen if you apply these changes.\nWhen you run terraform plan, Terraform reads your configuration and the current state, then compares them to determine what changes need to be made to achieve the desired state. The output of this command will display a list of actions Terraform will take, including creating, updating, or deleting resources.\nIt also provides information about which resources will be changed, added, and removed (if any). Additionally, it provides an estimate of the cost and time required to implement the changes.\nThe following image shows the output after running this command:\nterraform apply -auto-approve The terraform apply -auto-approve command applies the changes planned by Terraform without requiring additional confirmation from the user. This means Terraform will automatically apply the changes without needing any further confirmation.\nThe following image shows the output after running this command:\nSome resources may take time to create, but once they are created, you will receive an apply complete message. Check on AWS Console Access the following consoles to check the information of the resources you have created:\nVPC console link Codepipeline console link ECR console link ECS console link SNS topic console link EvenBridge rule console link Endpoint console link S3 bucket link ALB console link Information about codepipeline state changes is sent to the registered email. Remember to check your email early for your SNS services Subscription email.\nAccess the following information to see if the website has been built.\nContainer 1 Container 2 As we can see, the website is functioning properly, and the load balancer routes traffic to 2 containers.\nHave you noticed that here I use the DNS of the load balancer? But in the initial architecture, we also configured route53! Because following a workshop straight through won\u0026rsquo;t awaken your curiosity, so in the final module, I\u0026rsquo;ll let you write it yourself and deploy a domain name that you register. Challenge yourself and create a complete system for this picture. I believe you can do it if you\u0026rsquo;ve read this far.\nPush new code to the streamlit app repository Change any code or text in the directory containing the source code of the streamlit app and push it to GitHub. Return to the Codepipeline console to see if the pipeline is functioning correctly. Check the email response when the codepipeline changes state and check ECS Fargate services to see if tasks have been created or not.\nDestroy infrastructure Run the following command to destroy the infrastructure you provisioned.\nterraform destroy -auto-approve "},{"uri":"https://bibichannel.github.io/workshop-02/4-knowledgeterraform/4.1-overview/","title":"Overview","tags":[],"description":"","content":"1. Overview of Terraform Terraform is an open-source tool by HashiCorp, specialized in provisioning infrastructure. With Terraform, we simply write code and execute a few simple CLI commands, and it will create infrastructure for us.\nThe language Terraform uses is called HashiCorp Configuration Language (HCL).\nThe flow of Terraform is as follows: we write code, then execute CLI commands, and it will provision infrastructure. Once it\u0026rsquo;s done, it generates a state file to store the current infrastructure architecture. The state is a record of the current infrastructure that Terraform manages. It\u0026rsquo;s used to track resources created, modified, or deleted. The state also helps Terraform determine the changes needed to achieve the desired state of the infrastructure.\nState file: By default, Terraform stores state in a file named terraform.tfstate in the current working directory. This file contains information about all resources Terraform manages.\nState Backend: Terraform can store state on remote storage services like Amazon S3, Google Cloud Storage, Azure Blob Storage, or Terraform Cloud. This helps share state among team members and keeps it secure.\n2. How Terraform Connects to Clouds Terraform sends requests to the public APIs of clouds. On the cloud side, it authenticates these requests. Terraform uses providers to attach information during the authentication process. Providers can be Google Cloud Platform provider, AWS provider, Azure provider, etc. To support the authentication process, we need service accounts (IAM) to provide permissions. 3. Terraform Lifecycle When working with Terraform, it\u0026rsquo;s essential to understand these four basic commands:\nTerraform Init: Used to initialize the local environment. This command reads the scripts we\u0026rsquo;ve created, checks the version, and downloads necessary API packages for deployment. Run terraform init the first time or when adding some providers or public/private modules into our terraform code.\nTerraform Plan: Compares the state of the local environment and the one on cloud (specifically comparing within the state file). It provides an execution plan, such as changes before or after deployment. This process does not deploy anything to the cloud.\nTerraform Apply: Executes the deployment, pushing changes of the source code to the cloud. It creates state files.\nTerraform Destroy: As the name suggests, it deletes all the previously deployed infrastructure.\n4. Blocks in Terraform 1. Provider Block The Provider block is used to define and configure the service provider Terraform will interact with, such as AWS, Azure, Google Cloud,\u0026hellip;\n2. Terraform Block The Terraform block is used to configure global settings of Terraform, such as required version of Terraform, backend, and default providers.\n3. Resource Block The Resource block is used to create and manage resources within the service provider, such as EC2 instances, S3 buckets,\u0026hellip;\n4. Variable Block The Variable block is used to define input variables, allowing users to customize values in Terraform configuration.\n5. Module Block The Module block is used to call and use Terraform modules, facilitating configuration reuse and better source code organization.\n6. Data Block The Data block is used to query information about existing resources or services from the service provider without creating or changing any resources.\n7. Locals Block The Locals block is used to define local variables, simplifying and reusing complex values in Terraform configuration.\n8. Output Block The Output block is used to define output variables, providing useful information after infrastructure deployment. These outputs may include resource IDs, IP addresses, etc.\n9. Backend Block The Backend block is used to define and configure the backend, where Terraform state will be stored (such as in S3, Azure Blob Storage, etc.).\nThese blocks are written in .tf files, below are common .tf files for organizing Terraform code effectively.\n5. Files in Terraform 1. main.tf main.tf is the main file in Terraform configuration, containing resource definitions (resource blocks) and necessary configurations for infrastructure deployment (locals block, terraform block, module block,\u0026hellip;).\n2. variables.tf variables.tf is used to define input variables (variables block) that can be referenced in other configuration files. This helps in reusability and easy management of configurations.\n3. output.tf output.tf is used to define output variables (output block), providing useful information after infrastructure deployment. These outputs may include resource IDs, IP addresses,\u0026hellip;\n4. vars.tfbackend vars.tfbackend is commonly used to configure the backend for Terraform, where the Terraform state will be stored (such as in S3, Azure Blob Storage, etc.). This file is often included in the terraform init command to set up the backend.\n5. terraform.tfvars terraform.tfvars is used to provide default values for input variables defined in variables.tf. This file helps separate configuration and variable values, allowing easy configuration changes without editing the main configuration file.\n6. providers.tf providers.tf is used to define service providers (providers block) Terraform will interact with, such as AWS, Azure, Google Cloud, etc. This file may also contain configurations for providers, such as region, credentials, etc.\nSeparating these configurations helps maintain and manage Terraform projects easily, ensuring clear and organized configurations.\n"},{"uri":"https://bibichannel.github.io/workshop-02/3-setupgithub/3.1-createterraformrepo/","title":"Terraform IaC to AWS Cloud","tags":[],"description":"","content":"The source code for this part is available here: terraform-aws-continuous-docker-deployment-to-aws-fargate. You can fork it directly from my repository: With the Repository name matching the one you set when creating the role for OIDC in step 2.2.\nClick Create fork.\nIn the repository interface you just created or forked, go to settings.\nSelect Secrets and variables, then click Actions.\nWe will proceed to create 2 new repository secrets with the following content:\nTERRAFORM_BUCKET_NAME: the name of the bucket you created in step 2.3. CODESTAR_CONNECTION_ARN: containing the ARN of the connection we created in step 2.4. With these 2 secret variables, they will be used during the action execution when the GitHub workflow is activated.\nLet\u0026rsquo;s briefly explore the components in this repository The directory structure of this repo is as follows:\nHere we have 3 main directories:\nmodules: contains Terraform modules. .github: contains GitHub workflow files, used to run GitHub actions. static: contains image directories for writing Readme.md files. In addition to files with the .tf extension, which I\u0026rsquo;ll introduce later, we\u0026rsquo;ll focus on the following files:\nterraform-deploy.yml: is a YAML configuration file used in the pipeline of (CI/CD) to automatically deploy infrastructure using Terraform. Below is the content of the file. name: Deploy Infrastructure on: workflow_dispatch: inputs: actions: description: \u0026#34;Selecting actions for terraform\u0026#34; required: true default: \u0026#34;plan\u0026#34; type: choice options: - plan - apply - destroy environment: description: \u0026#34;Selecting evironments for terraform\u0026#34; required: true default: \u0026#34;staging\u0026#34; type: choice options: - staging - prod env: AWS_REGION : \u0026#34;us-east-1\u0026#34; permissions: id-token: write # This is required for requesting the JWT contents: read # This is required for actions/checkout jobs: build: runs-on: ubuntu-latest name: DEPLOY INFRASTRUCTURE environment: ${{ github.event.inputs.environment }} steps: - name: Check out repository code uses: actions/checkout@v3 - name: Configure AWS Credentials uses: aws-actions/configure-aws-credentials@v4 with: aws-region: ${{ env.AWS_REGION }} role-to-assume: arn:aws:iam::590183956208:role/github-oidc-role role-session-name: rolesession - name: Get account id run: | ACCOUNT_ID=$(aws sts get-caller-identity --query Account --output text) echo \u0026#34;AWS_ACCOUNT_ID=$ACCOUNT_ID\u0026#34; \u0026gt;\u0026gt; $GITHUB_ENV - name: Setup Terraform uses: hashicorp/setup-terraform@v3 with: terraform_version: 1.7.3 terraform_wrapper: false - name: Prepare Terraform env: PROJECT_NAME: \u0026#34;workshop-02\u0026#34; STAGE_NAME: ${{ github.event.inputs.environment }} TERRAFORM_BUCKET_NAME: ${{ secrets.TERRAFORM_BUCKET_NAME }} CODESTAR_CONNECTION_ARN: ${{ secrets.CODESTAR_CONNECTION_ARN }} YOUR_GITHUB_REPOSITORY: \u0026#34;GitHubOrg/GitHubRepo\u0026#34; YOUR_GITHUB_BRANCH: \u0026#34;main\u0026#34; run: | chmod +x setup.sh bash setup.sh terraform init -backend-config=./vars.tfbackend - name: Plan Terraform run: | terraform plan if: ${{ github.event.inputs.actions == \u0026#39;plan\u0026#39; }} - name: Aplly Terraform run: | terraform apply -auto-approve if: ${{ github.event.inputs.actions == \u0026#39;apply\u0026#39; }} - name: Destroy Terraform run: | terraform destroy -auto-approve if: ${{ github.event.inputs.actions == \u0026#39;destroy\u0026#39; }} With workflow_dispatch, it allows manual activation with inputs:\nactions: Choose Terraform actions (plan, apply, destroy). environment: Choose environment (staging, prod). Setting up permissions for using OIDC, used in the step named Configure AWS Credentials:\nid-token: write is the necessary permission to request JWT. With JWT to get temporary AWS authentication information. You can read more here: configure-aws-credentials Here, we pay attention to the important step named Prepare Terraform, in this step, we need to define the environment variables for the step, get and use variables from GitHub secrets.\nBesides the variables that you can understand by reading their names, there will be 2 variables representing the repository containing the source code for your application. YOUR_GITHUB_REPOSITORY: \u0026ldquo;GitHubOrg/GitHubRepo\u0026rdquo; YOUR_GITHUB_BRANCH: \u0026ldquo;main\u0026rdquo; Then we will run the setup.sh script and initialize Terraform with backend configuration. This part might be confusing for those who are not familiar with Terraform, but don\u0026rsquo;t worry, I\u0026rsquo;ll explain it later. About the setup.sh file. After being executed, this file will create 2 files: terraform.tfvars and vars.tfbackend, containing variables for setting up Terraform. If you don\u0026rsquo;t understand the contents of the workflow file, about GitHub action and its components, please read more about GitHub action to better understand.\nRegarding secret variables Let\u0026rsquo;s follow this flow to understand better:\nWhen the workflow is triggered, GitHub action will proceed to retrieve secret variables and decrypt them to access the variable values. Because the variables are encrypted, the process is secure.\nNext, the variables will be passed as follows:\nTo the Prepare terraform step, where we have defined the variables for this step, it will create 2 files terraform.tfvars and vars.tfbackend. These are files containing the necessary variables to set up and pass variables to Terraform modules for use. When we have provided all the input variables, Terraform will be executed and infrastructure will be provisioned on AWS Cloud. "},{"uri":"https://bibichannel.github.io/workshop-02/2-prerequisites/2.2-createoidc/","title":"Create OIDC","tags":[],"description":"","content":"In our architecture, we utilize a GitHub repository to perform infrastructure provisioning on AWS Cloud services. To grant GitHub the necessary permissions for this, we will need to have credentials for GitHub.\nThe simplest way, like using local, is to utilize the access key of our user account, provided to GitHub during the execution of Git actions. However, this requires creating a secret in GitHub, passing our AWS user credentials to a third party. During this process, if there are policies that need to be reset every 90 days, it\u0026rsquo;s time-consuming to update the secret key in GitHub.\nTherefore, we will employ a newer method to grant permissions. It will help us generate temporary credentials each time Git actions are run, enhancing security and manageability. That method is creating an OpenID Connect (OIDC) identity provider in IAM.\nCreating an OIDC in IAM is straightforward:\nIn the IAM interface, select Identity providers. Choose Add provider. In the Configure provider section, select OpenID Connect. Enter the URL of the IdP in the Provider URL: https://token.actions.githubusercontent.com. In the Audience field, input sts.amazonaws.com. Then, select Add provider. Here\u0026rsquo;s the information about the Identity providers we just created. Remember to save the Amazon Resource Name (ARN), as we\u0026rsquo;ll need it for role creation. You can find more information at: Create an OpenID Connect (OIDC) identity provider in IAM\nAfter creating the IAM OIDC identity provider, you need to create an IAM role. This role allows GitHub\u0026rsquo;s IdP to request temporary security credentials to access AWS.\nIn the IAM interface, select Roles, then choose Create role. At Select trusted entity, check Custom trust policy, then paste the following content into the editor. { \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Principal\u0026#34;: { \u0026#34;Federated\u0026#34;: providerArn }, \u0026#34;Action\u0026#34;: \u0026#34;sts:AssumeRoleWithWebIdentity\u0026#34;, \u0026#34;Condition\u0026#34;: { \u0026#34;StringEquals\u0026#34;: { \u0026#34;token.actions.githubusercontent.com:aud\u0026#34;: \u0026#34;sts.amazonaws.com\u0026#34; }, \u0026#34;StringLike\u0026#34;: { \u0026#34;token.actions.githubusercontent.com:sub\u0026#34;: \u0026#34;repo:GitHubOrg/GitHubRepo:*\u0026#34; } } } ] } For the Principal element, use the Amazon Resource Name (ARN) of the OIDC IdP created above. For the Condition element, use StringEquals to restrict permissions. For the StringLike element, use repo:GitHubOrg/GitHubRepo:*, for example, repo:bibichannel/terraform-project:*. This will limit IAM\u0026rsquo;s ability to assume a role for GitHub in the repository you specify, which is the best practice to limit entities that can assume roles associated with IAM IdP. You can find more information at: Configuring a role for GitHub OIDC identity provider.\nUnder Permissions policies, add the AdministratorAccess permission. This is for quick setup, but in reality, apply the principle of least privilege to increase the security of your account.\nName the role: github-oidc-role. Then, select Create role. With the IAM Role we\u0026rsquo;ve created, it will grant the GitHub repository access to AWS resources in our account. We\u0026rsquo;ll use this role during the GitHub action setup. Let\u0026rsquo;s move on to the next step.\n"},{"uri":"https://bibichannel.github.io/workshop-02/5-provisioningtesting/5.2-githubaction/","title":"Git action","tags":[],"description":"","content":"With provisioning infrastructure on GitHub Actions, it\u0026rsquo;s easier, and you don\u0026rsquo;t need to prepare anything.\nAccess the Terraform repository you created in step 3.1. Select Actions, click on Deploy Infrastructure, then choose Run workflow. Choose the necessary parameters for your needs and click Run workflow. You can access to view details, here I\u0026rsquo;m only running terraform plan.\nSelect apply for Selecting actions for terraform and rerun, check the result as in step 5.1.\nHere, I\u0026rsquo;m running a test run. But in reality, it will automatically run whenever the Terraform code is edited and pushed. You can try adding trigger conditions in the .github/workflows/terraform-deploy.yml file.\n"},{"uri":"https://bibichannel.github.io/workshop-02/2-prerequisites/","title":"Preparation ","tags":[],"description":"","content":"To practice this lab, you need to prepare the following items to ensure our code runs smoothly.\nOne more thing to note is that I will use the us-east-1 region throughout this workshop.\nContents Create User Groups and IAM User Create OIDC connection Create S3 bucket Create CodeStar Connection Install AWS and Terraform tools "},{"uri":"https://bibichannel.github.io/workshop-02/3-setupgithub/3.2-createstreamlitrepo/","title":"Streamlit application","tags":[],"description":"","content":"The source code for this part is available here: streamlit-app. You can fork it directly from my repository: With the Repository name matching the one you set for the YOUR_GITHUB_REPOSITORY variable in the terraform-deploy.yml file of the repository you created in step 3.1\nClick Create fork.\nLet\u0026rsquo;s briefly explore the components in this repository The directory structure of this repo is as follows:\nStreamlit is an open-source app framework that allows you to create websites quickly using Python without needing frontend experience. Dockerfile is a file to build the source code into an image with the following content: FROM public.ecr.aws/docker/library/python:3.9.19-slim-bullseye WORKDIR /app COPY main.py requirements.txt /app/ RUN pip3 install -r requirements.txt EXPOSE 8501 ENTRYPOINT [\u0026#34;streamlit\u0026#34;, \u0026#34;run\u0026#34;, \u0026#34;main.py\u0026#34;, \u0026#34;--server.port=8501\u0026#34;, \u0026#34;--server.address=0.0.0.0\u0026#34;] Here I\u0026rsquo;m using a base image built from the AWS public registry. Can I use images from the docker registry? During my work, I tried to use images from the docker registry, but I encountered an error immediately:\n\u0026ldquo;toomanyrequests: You have reached your pull rate limit. You may increase the limit by authenticating and upgrading: https://www.docker.com/increase-rate-limit\u0026rdquo; Docker limits the pulling of images from its public registry, it uses the IP address to authenticate users and limits the pulling speed based on the IP address. For anonymous users, the pulling limit is set to 100 pulls every 6 hours per IP address. So, we will use the Amazon ECR public registry to pull the image.\nbuildspec.yml is the file for AWS CodeBuild to run the build. You can refer to more components in the buildspec file at Build specification reference for CodeBuild. The file has the following content: version: 0.2 phases: pre_build: commands: # Retrieve an authentication token and authenticate your Docker client to your registry - RESULT=$(aws ecr get-login-password --region $AWS_REGION | docker login --username AWS --password-stdin $REPOSITORY_URL) - echo $RESULT build: commands: # Build the Docker image - docker build -t streamlit-app:latest . - SHORT_VERSION=$(echo \u0026#34;$CODEBUILD_RESOLVED_SOURCE_VERSION\u0026#34; | cut -c 1-8) - docker tag streamlit-app:latest $REPOSITORY_URL:$SHORT_VERSION - docker tag streamlit-app:latest $REPOSITORY_URL:latest - echo $SHORT_VERSION - echo $REPOSITORY_URL post_build: commands: # Push docker images to ECR - docker push $REPOSITORY_URL:latest - docker push $REPOSITORY_URL:$SHORT_VERSION - printf \u0026#39;[{\u0026#34;name\u0026#34;:\u0026#34;streamlit-app\u0026#34;,\u0026#34;imageUri\u0026#34;:\u0026#34;%s\u0026#34;}]\u0026#39; $REPOSITORY_URL:$SHORT_VERSION \u0026gt; imagedefinitions.json artifacts: files: imagedefinitions.json During the build process, there will be 3 phases: pre_build: performs login to the AWS Elastic Container Registry. build: builds the image and tags it. I will assign 2 tags to the built image. post_build: pushes the image to AWS ECR and creates an artifact imagedefinitions.json containing information about the container name and container URI, used for deploying to ECS. "},{"uri":"https://bibichannel.github.io/workshop-02/4-knowledgeterraform/4.2-terraformflow/","title":"Terrafrom flow","tags":[],"description":"","content":"In the previous section, we briefly explored the components of Terraform. Now, let\u0026rsquo;s delve into more detail about the Terraform source code provided in this workshop.\nI\u0026rsquo;ve organized the creation of resources in Terraform into modules for easy management. There are a total of 11 modules:\nVPC: Contains basic resources to create a VPC in the cloud, such as subnets, route tables, security groups, etc.\nALB, Cloudwatch, CodeBuild, Codepipeline, ECR, ECS, Endpoint. EventBridge, S3, SNS: Just by reading the names, you can probably guess which services they provide.\nEach module will have the following files: main.tf, variables.tf, output.tf, and role.tf (contains resources to create permissions for services, depending on whether the resource requires it or not). Regarding the purpose of each file, I\u0026rsquo;ve outlined it in section 4.1.\nSo how do you find the necessary resources, arguments, and attributes for each block? You can find them here: AWS Provider Registry Docs. This is the latest documentation available at the time I wrote this workshop. It contains all the information you need to write Terraform code and create the services you want.\n"},{"uri":"https://bibichannel.github.io/workshop-02/2-prerequisites/2.3-createbucket/","title":"Create S3 bucket","tags":[],"description":"","content":"We will create a bucket to store the state file for Terraform. I\u0026rsquo;ll provide a detailed explanation in section 4.\nIn the Create bucket interface: AWS region: Choose the S3 bucket region based on the region where we are conducting the lab. Currently, I\u0026rsquo;m conducting the lab in the N. Virginia (us-east-1) region. Bucket Type: For the scope of this lab, we\u0026rsquo;ll select General purpose. Enter a Bucket name, ensuring it\u0026rsquo;s unique. You can choose any name; here, I\u0026rsquo;ll create a bucket named backend-tf-bibichannel. AWS S3 can be accessed publicly, and it provides us with access to its buckets and objects via the REST API. In essence, the path must adhere to DNS and cannot have two identical domain names. Hence, bucket names need to be unique to access groups and objects through the REST API endpoint.\nBelow, we\u0026rsquo;ll keep the default configurations and click Create bucket.\n"},{"uri":"https://bibichannel.github.io/workshop-02/3-setupgithub/","title":"Prepare github repository","tags":[],"description":"","content":"In this workshop, we\u0026rsquo;ll need 2 GitHub repositories:\nThe first repository will store Terraform code for provisioning resources to AWS through Git actions. This will allow us to downgrade versions whenever configuration changes cause issues through version management on GitHub. Additionally, it enables us to collaborate effectively. The second repository will store the source code of the Streamlit app. Whenever code is pushed to this repository, it will trigger a push event through a webhook to trigger CodePipeline, which will build and deploy it to ECS Fargate. Contents Terraform IaC to AWS Cloud Streamlit application "},{"uri":"https://bibichannel.github.io/workshop-02/2-prerequisites/2.4-createcodestar/","title":"Create CodeStar Connection","tags":[],"description":"","content":"Let\u0026rsquo;s pay attention to the architecture again. We notice there\u0026rsquo;s a GitHub repository that pushes events of changes to GitHub to trigger CodePipeline.\nTherefore, we also need a tool to connect GitHub to AWS. AWS acknowledges this and provides us with a tool called CodeStar Connection.\nAccess CodePipeline. Choose Connections. Select Create connection. In the Select a provider section, check GitHub. Set Connection name to workshop-02. Choose Connect to GitHub. Under GitHub Apps, click Install a new app. You\u0026rsquo;ll be redirected to the login page of your GitHub account. Log in and authorize AWS. Once created, save the ARN of our connection to use during the Terraform code writing process. Hold on a minute!!! This task employs Terraform to provision infrastructure on AWS, so why don\u0026rsquo;t we write code to create these services? Initially, I had this thought too, but when we write code to create the CodeStar Connection, upon creation, the service will be in a PENDING state, and we\u0026rsquo;ll need to manually intervene in the AWS Console to successfully connect it. This would disrupt automation, making the automation process discontinuous. So, let\u0026rsquo;s create this service first to ensure the IaC process runs smoothly.\n"},{"uri":"https://bibichannel.github.io/workshop-02/4-knowledgeterraform/","title":"Knowledge Terraform","tags":[],"description":"","content":"Before provisioning IaC on the AWS Cloud, I\u0026rsquo;ll give a quick introduction to Terraform and its components for those who are unfamiliar.\nContent Overview Terraform flow "},{"uri":"https://bibichannel.github.io/workshop-02/2-prerequisites/2.5-installtools/","title":"Install tools","tags":[],"description":"","content":"In this section, we\u0026rsquo;ll need to install AWS CLI and Terraform CLI on your local machine.\nFor AWS CLI, please refer to the following link: Install to the latest version of the AWS CLI After you have installed AWS CLI, let\u0026rsquo;s create a profile to use. We\u0026rsquo;ll use the terraform user we created in the previous step .\nAccess PowerShell and type the following command.\naws configure --profile \u0026lt;profile-name\u0026gt; You can set any Profile name you want. I\u0026rsquo;ll set it as terraform. Enter the necessary information. To use the profile we just imported, set the environment variable for it: #### CMD Windows set AWS_PROFILE=profile_name #### PowerShell Windows $env:AWS_PROFILE=\u0026#39;profile_name\u0026#39; #### Linux or Mac export AWS_PROFILE=profile_name Check again to see if the profile is being used: aws sts get-caller-identity The result after running the above commands. Install Terraform Follow the instructions below to install Terraform on Windows: Install Terraform for Windows After installation, check the version you are using. Currently, my version is v1.7.3. Since Terraform is open-source, it is supported and updated by the community very quickly. So when you perform this workshop, your version may be outdated, and some features in the provided Terraform code may be outdated. But don\u0026rsquo;t worry, because the documentation is constantly updated according to the version, so you can refer to it and make modifications as needed.\n"},{"uri":"https://bibichannel.github.io/workshop-02/5-provisioningtesting/","title":"Provisioning &amp; Testing","tags":[],"description":"","content":"In this part, I will guide you to run code on Local and on Git Action.\nContent Local Git Action "},{"uri":"https://bibichannel.github.io/workshop-02/6-cleanup/","title":"Resource Cleanup","tags":[],"description":"","content":"Sure, here are the steps to sequentially delete the resources:\nIf you are using Terraform locally, use the following command to destroy the resources: terraform destroy -auto-approve Run the workflow with the parameter destroy in Selecting actions for terraform to delete the provisioned infrastructure.\nDelete the repository if necessary.\n"},{"uri":"https://bibichannel.github.io/workshop-02/categories/","title":"Categories","tags":[],"description":"","content":""},{"uri":"https://bibichannel.github.io/workshop-02/tags/","title":"Tags","tags":[],"description":"","content":""}]